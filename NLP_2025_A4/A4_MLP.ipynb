{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 6.0 MB/s eta 0:00:00\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "     -------------------------------------- 302.2/302.2 kB 9.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\n.jintaganon\\appdata\\local\\anaconda3\\envs\\nlp_a4\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2024.4.16-cp37-cp37m-win_amd64.whl (269 kB)\n",
      "     -------------------------------------- 269.6/269.6 kB 5.5 MB/s eta 0:00:00\n",
      "Collecting click\n",
      "  Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "     ---------------------------------------- 98.2/98.2 kB 2.8 MB/s eta 0:00:00\n",
      "Collecting importlib-metadata\n",
      "  Downloading importlib_metadata-6.7.0-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\n.jintaganon\\appdata\\local\\anaconda3\\envs\\nlp_a4\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\n.jintaganon\\appdata\\local\\anaconda3\\envs\\nlp_a4\\lib\\site-packages (from importlib-metadata->click->nltk) (4.4.0)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\n",
      "Installing collected packages: zipp, regex, joblib, importlib-metadata, click, nltk\n",
      "Successfully installed click-8.1.8 importlib-metadata-6.7.0 joblib-1.3.2 nltk-3.8.1 regex-2024.4.16 zipp-3.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.5.3-cp37-cp37m-win_amd64.whl (7.2 MB)\n",
      "     ---------------------------------------- 7.2/7.2 MB 5.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\n.jintaganon\\appdata\\local\\anaconda3\\envs\\nlp_a4\\lib\\site-packages (from matplotlib) (2.9.0)\n",
      "Collecting pyparsing>=2.2.1\n",
      "  Downloading pyparsing-3.1.4-py3-none-any.whl (104 kB)\n",
      "     -------------------------------------- 104.1/104.1 kB 3.0 MB/s eta 0:00:00\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
      "     -------------------------------------- 965.4/965.4 kB 7.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\n.jintaganon\\appdata\\local\\anaconda3\\envs\\nlp_a4\\lib\\site-packages (from matplotlib) (1.21.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\n.jintaganon\\appdata\\local\\anaconda3\\envs\\nlp_a4\\lib\\site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\n.jintaganon\\appdata\\local\\anaconda3\\envs\\nlp_a4\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.5-cp37-cp37m-win_amd64.whl (55 kB)\n",
      "     ---------------------------------------- 55.8/55.8 kB 1.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\n.jintaganon\\appdata\\local\\anaconda3\\envs\\nlp_a4\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib) (4.4.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\n.jintaganon\\appdata\\local\\anaconda3\\envs\\nlp_a4\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, matplotlib\n",
      "Successfully installed cycler-0.11.0 fonttools-4.38.0 kiwisolver-1.4.5 matplotlib-3.5.3 pyparsing-3.1.4\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\n.jintaganon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import nltk, string, random, numpy, os\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "from nltk.corpus import brown\n",
    "os.makedirs('./results', exist_ok=True)\n",
    "nltk.download('brown')\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "numpy.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "In the warm-up of this assignment, we will see how Neural Networks (NN) handle natural language data.   \n",
    "The warm-up focuses on a simple Multilayer Perceptron (MLP), also known as a fully connected Neural Network. The data we'll use is the first 5000 unique words of the Brown corpus.\n",
    "\n",
    "### Dataset\n",
    "To train the model, we will have to represent the input words to the model in some way. Since models solely work with numbers, the words will have to be converted into numerical form.  \n",
    "For this assignment, we will focus on predicting individual words from the dataset given the input of the model. The input will be the target word split up into individual letters. To represent these individual letters we will give the model a vector of 26 positions (26 letters in the English alphabet). Initially, this vector is filled with zeros and for every occuring letter in the word we change the value to 1 in that position. For instance, in the word `apple', we have 1 a, 1 e, 1 l and 2 p. The vector will then represent the word as:  \n",
    "```[1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,2,0,0,0,0,0,0,0,0,0,0]```  \n",
    "You will have to implement this algorithm together with loading the Brown dataset and taking the first 5000 unique words. Implement a way to store the indexes of the unique words as a dictionary where the word is the key and the index is the value as well as the target list (which will be just the indexes of the words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main task translation = give 26 positions vector that indicates letters consisted in word, no positional vector, make NN spell the word like doing scrabble. <br> \n",
    "also do = create storage of the index of these words, where key = word, value = index <br>\n",
    "also do = target list -> [indexes of words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaDataset(Dataset):\n",
    "    def __init__(self) -> None:\n",
    "        # Load the NLTK Brown corpus and store the first 5000 unique words of the corpus in self.data\n",
    "        words = brown.words()\n",
    "        lowercase_words = [word.lower() for word in words] # Convert all words to lowercase\n",
    "        seen = set()\n",
    "        unique_words = [] # Create a list of unique words\n",
    "        for word in lowercase_words:\n",
    "            if word not in seen and len(word) > 1 and all(letter in string.ascii_lowercase for letter in word):\n",
    "                unique_words.append(word)\n",
    "                seen.add(word)\n",
    "            if len(unique_words) == 5000:\n",
    "                break\n",
    "    \n",
    "        self.data = unique_words # Load the first 5000 words from the Brown corpus\n",
    "        self.word_to_idx = {word: idx for idx, word in enumerate(self.data)} # Convert the unique words to an index dictionary {word: index}\n",
    "        self.targets = list(range(len(self.data))) # Make these indexes the target values\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # Torch requires the implementation of the length function to calculate the number of instances in the dataset. Find a way to implement this\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.targets[index]\n",
    "\n",
    "        # Apply processing to turn the word (stored in x) into a numeric vector of 26 numbers, counting the occurences of the letters.\n",
    "        # Example: apple would become [1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,2,0,0,0,0,0,0,0,0,0,0], counting 2 occurences for the letter p and zero for letters that do not occur.\n",
    "       \n",
    "        vector = [0] * 26  # Initialize a vector of size 26 with all zeros\n",
    "        for letter in x:\n",
    "            if letter not in string.ascii_lowercase:\n",
    "                raise ValueError(f\"Invalid character '{letter}' in word '{x}'\")\n",
    "            vector[ord(letter) - ord('a')] += 1  # using ord to get the index of the letter in the alphabet as an index of position in vector, then increment value\n",
    "        x = vector\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "        return x, y\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test validity of the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "(tensor([0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0.]), tensor(0))\n",
      "torch.Size([26])\n",
      "torch.Size([])\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "dataset = AlphaDataset()\n",
    "print(len(dataset))  # Should be 5000\n",
    "print(dataset[0])    # Should return a tuple: (tensor, label)\n",
    "print(dataset[0][0].shape)  # Should be (26,)\n",
    "print(dataset[0][1].shape)  # Should be (1,)\n",
    "print(dataset.data[0])  # Should return the first word in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron\n",
    "In the following section, you will implement a MLP. The goal is to implement this MLP with 1 input layer, 2 hidden layers, and 1 output layer.\n",
    "With PyTorch, the linear layer is most suitable for this. When you create a linear layer, you define the input and output size of the layer, effectively creating two linear neuron layers. This is useful to know since we only need to create 3 linear layer classes to have the 4 layers we want.\n",
    "The hidden size is stored as a list where the first value will be 256 and the second value will be 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: List[int], output_size: int) -> None:\n",
    "        super().__init__()\n",
    "        # Implement the neural network layers, the activation function is already defined\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size[0])\n",
    "        self.hidden_layer = nn.Linear(hidden_size[0], hidden_size[1])\n",
    "        self.output_layer = nn.Linear(hidden_size[1], output_size)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # In the forward pass the model will calculate the gradients as well as the probabilities of the result occuring given its input.\n",
    "        # Implement the missing layers\n",
    "\n",
    "        x = self.input_layer(x) # Implement input layer\n",
    "        x = self.activation(x)\n",
    "        x = self.hidden_layer(x) # Implement hidden layer\n",
    "        x = self.activation(x)\n",
    "        x = self.output_layer(x) # Implement the output layer\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the hyperparameters\n",
    "These are the hyperparameters used for the model, they define the layout of the model as well as the performance:\n",
    "- batch_size, defines the number of instances the model sees at one time.\n",
    "- learning_rate, defines the change rate of the gradient descent.\n",
    "- input_size, the number of input neurons for the model, the number of letters in the alphabet\n",
    "- hidden_size, the number of neurons in the hidden layer\n",
    "- output_size, the number of neurons in the output layer, for us this is the number of unqiue words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "learning_rate = 1e-5\n",
    "input_size = 26\n",
    "hidden_size = [256, 512]\n",
    "output_size = 5000\n",
    "device = 'cpu' # If you have an m1 macbook use: 'mbp', if you have an NVIDIA GPU use: 'cuda:0' else leave as is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset and the dataloader\n",
    "dataset = AlphaDataset()\n",
    "\n",
    "# For the final evaluation of the model we will use 20% of the data for testing. Testing is only ever done after hyperparameter tuning.\n",
    "# Split sizes (80% train, 20% test)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# To evaluate our model we want to take 10% of the dataset for validation, this is similar to the testset, rather this data we can use during hyperparameter tuning.\n",
    "# The validation and test data is never trained on and is unseen data for the model, making it closer to a production setting.\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # Shuffling ensures the model does not overfit on ordering of the data.\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False) # This data does not need to be shuffled\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)  #T his data does not need to be shuffled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preperation for training\n",
    "Here we load the model into memory, apply it to the selected device and define the optimizer. The optimizer guides the model to the best possible state it can be in through Gradient descent.\n",
    "Lastly, the loss function is defined, this defines how well the model performs, based on this number the model knows how it should change its weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(input_size, hidden_size, output_size)\n",
    "model.to(device) # Tell de model which accelerator to use (Macbook GPU, NVIDIA GPU or CPU)\n",
    "\n",
    "# In Neural Networks optimizers handle the efficient training through gradient descent, we will use Adam\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "# The loss function defines how well the model is performing, if the loss is low the model is rewarded, if it is high the model is punished.\n",
    "# Since we are dealing with a classification task we will use Cross Entropy\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "In the next block the training block is already defined. This is a standard way to train the model for 50 epochs (50 times it will see the dataset). Each time it does one epoch we also go over the validationset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training_loss: 8.545825958251953\n",
      "Validation_loss: 1.0697916269302368\n",
      "Training_loss: 8.472640037536621\n",
      "Validation_loss: 1.080784502029419\n",
      "Training_loss: 8.361361503601074\n",
      "Validation_loss: 1.0989296412467957\n",
      "Training_loss: 8.224590301513672\n",
      "Validation_loss: 1.1296512413024902\n",
      "Training_loss: 8.04572868347168\n",
      "Validation_loss: 1.1752298665046692\n",
      "Training_loss: 7.941718101501465\n",
      "Validation_loss: 1.231899130344391\n",
      "Training_loss: 7.761636734008789\n",
      "Validation_loss: 1.2939287090301514\n",
      "Training_loss: 7.4719953536987305\n",
      "Validation_loss: 1.3572372198104858\n",
      "Training_loss: 7.459007263183594\n",
      "Validation_loss: 1.4203842663764954\n",
      "Training_loss: 7.3815741539001465\n",
      "Validation_loss: 1.483384714126587\n",
      "Training_loss: 7.579215049743652\n",
      "Validation_loss: 1.5469173002243042\n",
      "Training_loss: 7.425631523132324\n",
      "Validation_loss: 1.6118607854843139\n",
      "Training_loss: 7.056421756744385\n",
      "Validation_loss: 1.6786346244812012\n",
      "Training_loss: 7.169504642486572\n",
      "Validation_loss: 1.7475782990455628\n",
      "Training_loss: 7.079358100891113\n",
      "Validation_loss: 1.8187217807769775\n",
      "Training_loss: 6.712723255157471\n",
      "Validation_loss: 1.89221684217453\n",
      "Training_loss: 6.714460849761963\n",
      "Validation_loss: 1.9683060932159424\n",
      "Training_loss: 6.382720470428467\n",
      "Validation_loss: 2.0467820310592653\n",
      "Training_loss: 6.1915740966796875\n",
      "Validation_loss: 2.127744023799896\n",
      "Training_loss: 6.079456806182861\n",
      "Validation_loss: 2.2112436509132385\n",
      "Training_loss: 5.822981834411621\n",
      "Validation_loss: 2.296541647911072\n",
      "Training_loss: 5.633365154266357\n",
      "Validation_loss: 2.3841498708724975\n",
      "Training_loss: 5.420633316040039\n",
      "Validation_loss: 2.4730705213546753\n",
      "Training_loss: 6.177323818206787\n",
      "Validation_loss: 2.5639508676528933\n",
      "Training_loss: 5.965070724487305\n",
      "Validation_loss: 2.6568211269378663\n",
      "Training_loss: 5.200663089752197\n",
      "Validation_loss: 2.7507568645477294\n",
      "Training_loss: 5.142460823059082\n",
      "Validation_loss: 2.846059617996216\n",
      "Training_loss: 5.244842529296875\n",
      "Validation_loss: 2.9421218156814577\n",
      "Training_loss: 5.048605442047119\n",
      "Validation_loss: 3.0394170475006104\n",
      "Training_loss: 4.823433876037598\n",
      "Validation_loss: 3.137672176361084\n",
      "Training_loss: 5.067227363586426\n",
      "Validation_loss: 3.2363846063613892\n",
      "Training_loss: 4.507806777954102\n",
      "Validation_loss: 3.3359425926208495\n",
      "Training_loss: 4.406042575836182\n",
      "Validation_loss: 3.435217261314392\n",
      "Training_loss: 4.592220783233643\n",
      "Validation_loss: 3.5348984718322756\n",
      "Training_loss: 4.508082389831543\n",
      "Validation_loss: 3.6354547786712645\n",
      "Training_loss: 4.191458225250244\n",
      "Validation_loss: 3.734774112701416\n",
      "Training_loss: 4.175106525421143\n",
      "Validation_loss: 3.8342722034454346\n",
      "Training_loss: 4.066253185272217\n",
      "Validation_loss: 3.9339874267578123\n",
      "Training_loss: 4.11642599105835\n",
      "Validation_loss: 4.034290962219238\n",
      "Training_loss: 3.4285144805908203\n",
      "Validation_loss: 4.133372101783753\n",
      "Training_loss: 3.246304512023926\n",
      "Validation_loss: 4.2323747253417965\n",
      "Training_loss: 3.6877105236053467\n",
      "Validation_loss: 4.331860332489014\n",
      "Training_loss: 3.3924663066864014\n",
      "Validation_loss: 4.430566563606262\n",
      "Training_loss: 2.9946305751800537\n",
      "Validation_loss: 4.528943347930908\n",
      "Training_loss: 3.0630252361297607\n",
      "Validation_loss: 4.626605749130249\n",
      "Training_loss: 3.201005697250366\n",
      "Validation_loss: 4.724088487625122\n",
      "Training_loss: 2.5071074962615967\n",
      "Validation_loss: 4.820736255645752\n",
      "Training_loss: 2.853734254837036\n",
      "Validation_loss: 4.917004318237304\n",
      "Training_loss: 2.3559796810150146\n",
      "Validation_loss: 5.013584547042846\n",
      "Training_loss: 2.589275360107422\n",
      "Validation_loss: 5.108757600784302\n"
     ]
    }
   ],
   "source": [
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "for epoch in range(50): # Train for 50 epochs\n",
    "    model.train() # Enforce model training\n",
    "    for step, (x, y) in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        output = model(x)\n",
    "\n",
    "        loss = loss_fn(output, y)\n",
    "\n",
    "        training_losses.append(loss.item())\n",
    "        print(loss.item(), end='\\r')\n",
    "\n",
    "        loss.backward() # Calculate gradients\n",
    "\n",
    "        optimizer.step() # Reward the model\n",
    "        optimizer.zero_grad() # Clean the gradients\n",
    "    print('Training_loss:', loss.item())\n",
    "\n",
    "    model.eval() # After every training epoch we want to see the model's performance on the validation data\n",
    "    with torch.no_grad(): # In validation we dont need gradients so we tell torch to not calculate them\n",
    "        total_val_loss = 0\n",
    "        for step, (x, y) in enumerate(val_loader):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            output = model(x)\n",
    "\n",
    "            loss = loss_fn(output, y)\n",
    "\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "        validation_losses.append(total_val_loss / len(val_dataset))\n",
    "        print('Validation_loss:', validation_losses[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing\n",
    "Below we can print the training statistics, the training loss should be going down while the validation loss should be going up. What does it mean that the validation loss increases?\n",
    "\n",
    "[Your answer here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.plot(training_losses)\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Training Loss')\n",
    "plt.savefig('./results/training_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(validation_losses)\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Validation Loss')\n",
    "plt.savefig('./results/validation_loss.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing\n",
    "Belowe we want to analyze how the model functions based on the test data. What stands out from these results? Was the result correct? What is the main difference between the words?\n",
    "\n",
    "[Your answer here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_words(dataset, n, model, ds, verbose = True):\n",
    "    if n == -1:\n",
    "        n = len(dataset)\n",
    "    x = [dataset.__getitem__(i)[0] for i in range(n)]\n",
    "    y = [dataset.__getitem__(i)[1] for i in range(n)]\n",
    "    target_words = [{idx: word for word, idx in ds.word_to_idx.items()}[_.item()] for _ in y]\n",
    "    out = [nn.functional.softmax(model(wrd)).argmax() for wrd in x]\n",
    "    predicted_words = [{idx: word for word, idx in ds.word_to_idx.items()}[_.item()] for _ in out]\n",
    "    width = max(len(word) for word in target_words) + 5\n",
    "    if verbose:\n",
    "        print('\\n'.join([f'target: {t.ljust(width)} predicted: {p}' for t, p in zip(target_words, predicted_words)]))\n",
    "    return target_words, predicted_words\n",
    "\n",
    "_,_ = get_n_words(test_dataset, 10, model, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are some words incorrectly predicted?\n",
    "\n",
    "[Your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code + Written\n",
    "Research the Jaccard similarity metric for calculating the difference between the predicted word and the target word. Reference your sources and implement this metric in your code below. You can use the get_n_words function with n=-1 to get all the predicted and target words. Compare this method to similarity as measured with one of the word vector methods from A2, in writing and/or code.\n",
    "\n",
    "[Your answer here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_a4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
